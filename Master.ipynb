{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxX4BRUsJXw7"
      },
      "source": [
        "#Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import math\n",
        "from itertools import product, combinations\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from copy import copy, deepcopy\n",
        "from collections import deque\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import pickle as pkl\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import logging\n",
        "import datetime\n",
        "from tensorflow.keras.regularizers import l2, l1\n",
        "from tensorflow.keras.constraints import unit_norm\n",
        "from tensorflow.keras.layers import LayerNormalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a Portfolio Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the PortfolioEnv Class\n",
        "class PortfolioEnv():\n",
        "    def __init__(self, dates, datasets, n):\n",
        "        self.dates = dates\n",
        "        self.datasets = datasets\n",
        "        self.n = n\n",
        "        self.process_big_pt()\n",
        "        self.mm_scaler = MinMaxScaler()\n",
        "        self.process_small_pt()\n",
        "\n",
        "        self.initial_pt = 1000000  # 1 million\n",
        "        self.c_minus = 0.0025  # 0.25% selling transaction cost\n",
        "        self.c_plus = 0.0025   # 0.25% buying transaction cost\n",
        "        self.delta = 10000     # 10,000 trading size\n",
        "        self.current_epsilon = 1\n",
        "        \n",
        "        self.process_actions()\n",
        "        self.action_shape = self.actions.shape[0]\n",
        "        self._episode_ended = False\n",
        "        \n",
        "    def reset(self):\n",
        "        # Initialization of portfolio\n",
        "        self.pt = self.initial_pt  # Start with 1 million\n",
        "        self.wt = np.array([0.25, 0.25, 0.25, 0.25], dtype=np.float64)  # Equal weight portfolio\n",
        "        self.wt = self.wt / np.sum(self.wt) \n",
        "\n",
        "        self.current_tick = 0\n",
        "        self.episode_ended = False\n",
        "\n",
        "        # Use index 0 for the first indicator (closing price) - FIXED\n",
        "        ktc = self.big_pt[self.current_tick, 0, :, 0]\n",
        "        wt_prime = (self.wt*self.phi(ktc)) / (np.dot(self.wt, self.phi(ktc)))\n",
        "\n",
        "        return {'big_xt': np.array(self.small_pt[self.current_tick]), 'wt_prime': wt_prime}\n",
        "\n",
        "    def process_actions(self):\n",
        "        asset_number = len(self.datasets)\n",
        "        action_number = len(self.datasets)\n",
        "\n",
        "        seq = np.arange(asset_number)\n",
        "        actions = []\n",
        "\n",
        "        for c in product(seq, repeat=action_number):\n",
        "            actions.append(c)\n",
        "\n",
        "        self.actions = np.array(actions)\n",
        "\n",
        "    def find_action_index(self, action):\n",
        "        for ind, a in enumerate(self.actions):\n",
        "            if np.array_equal(a, action):\n",
        "                return ind\n",
        "        return None\n",
        "\n",
        "    def process_big_pt(self):\n",
        "        \"\"\"Create proper tensor structure as described in Figure 1\"\"\"\n",
        "        datasets = self.datasets\n",
        "        date_start = self.dates[0]\n",
        "        date_end = self.dates[1]\n",
        "\n",
        "        dfs = []\n",
        "        for d in datasets:\n",
        "            ticker = yf.Ticker(d)\n",
        "            # Get historical market data\n",
        "            df_ = ticker.history(start=date_start, end=date_end, interval=\"1d\")\n",
        "            df_.rename(mapper={\n",
        "                \"Close\": d+\"_close\",\n",
        "                \"Open\": d+\"_open\",\n",
        "                \"High\": d+\"_high\",\n",
        "                \"Low\": d+\"_low\",\n",
        "                \"Volume\": d+\"_volume\"\n",
        "            }, inplace=True, axis=1)\n",
        "            if \"Dividends\" in df_.columns:\n",
        "                df_.drop(axis=1, labels=[\"Dividends\", \"Stock Splits\"], inplace=True)\n",
        "            dfs.append(df_)\n",
        "\n",
        "        final_df = pd.concat(dfs, axis=1)\n",
        "        final_df.dropna(inplace=True)\n",
        "\n",
        "        self.final_df = final_df\n",
        "\n",
        "        n = self.n\n",
        "        I = len(self.datasets)\n",
        "    \n",
        "        # Create the technical indicators per asset\n",
        "        indicators = []\n",
        "        for indicator_type in ['c', 'o', 'h', 'l', 'v']:  # 5 indicators\n",
        "            asset_indicators = []\n",
        "            for d in self.datasets:  # For each asset\n",
        "                if indicator_type == 'c':  # Close price change\n",
        "                    asset_close = self.final_df[d+\"_close\"].values\n",
        "                    asset_prev_close = self.final_df[d+\"_close\"].shift().values\n",
        "                    K = (asset_close - asset_prev_close) / asset_prev_close\n",
        "                elif indicator_type == 'o':  # Open price ratio\n",
        "                    asset_prev_close = self.final_df[d+\"_close\"].shift().values\n",
        "                    asset_open = self.final_df[d+\"_open\"].values\n",
        "                    K = (asset_open - asset_prev_close) / asset_prev_close\n",
        "                elif indicator_type == 'h':  # High price ratio\n",
        "                    asset_close = self.final_df[d+\"_close\"].values\n",
        "                    asset_high = self.final_df[d+\"_high\"].values\n",
        "                    K = (asset_close - asset_high) / asset_high\n",
        "                elif indicator_type == 'l':  # Low price ratio  \n",
        "                    asset_close = self.final_df[d+\"_close\"].values\n",
        "                    asset_low = self.final_df[d+\"_low\"].values\n",
        "                    K = (asset_close - asset_low) / asset_low\n",
        "                elif indicator_type == 'v':  # Volume change\n",
        "                    asset_prev_volume = self.final_df[d+\"_volume\"].shift().values\n",
        "                    asset_volume = self.final_df[d+\"_volume\"].values\n",
        "                    K = (asset_volume - asset_prev_volume) / asset_prev_volume\n",
        "\n",
        "                K = K[1:]  # Remove first NaN value\n",
        "                # Create time windows of size n\n",
        "                K_windows = [K[i:i+n] for i in range(len(K)-n+1)]\n",
        "                asset_indicators.append(K_windows)\n",
        "            indicators.append(asset_indicators)\n",
        "\n",
        "        # Convert to numpy array and reshape according to Figure 1\n",
        "        indicators = np.array(indicators)\n",
        "        # Reshape to [time_steps, window_size(n), assets(I), indicators(5)]\n",
        "        self.big_pt = indicators.transpose(2, 3, 1, 0)\n",
        "        print(f\"Technical indicators tensor shape: {self.big_pt.shape}\")\n",
        "\n",
        "    def process_small_pt(self):\n",
        "        \"\"\"Process technical indicators for small state representation\"\"\"\n",
        "        small_pt = []\n",
        "\n",
        "        # Step 1: Aggregate all data for fitting\n",
        "        flattened_all = []\n",
        "        for big_xt in self.big_pt:\n",
        "            big_xt = big_xt.swapaxes(0, 1).swapaxes(1, 2)\n",
        "            flattened_all.append(big_xt.reshape(-1, big_xt.shape[-1]))  # (20, 5)\n",
        "\n",
        "        all_data = np.vstack(flattened_all)  # shape: (num_samples * 20, 5)\n",
        "        self.mm_scaler.fit(all_data)\n",
        "\n",
        "        # Step 2: Transform each sample using the fitted scaler\n",
        "        for big_xt in self.big_pt:\n",
        "            big_xt = big_xt.swapaxes(0, 1).swapaxes(1, 2)  # shape: (20, 5)\n",
        "            big_xt_reshaped = big_xt.reshape(-1, big_xt.shape[-1])\n",
        "            big_xt_scaled = self.mm_scaler.transform(big_xt_reshaped).reshape(big_xt.shape)\n",
        "            small_pt.append(big_xt_scaled)\n",
        "\n",
        "        self.small_pt = np.array(small_pt)\n",
        "        print(f\"Scaled indicators tensor shape: {self.small_pt.shape}\")\n",
        "\n",
        "    def get_30day_volatility(self):\n",
        "        \"\"\"Calculate market volatility for adaptive exploration\"\"\"\n",
        "        if self.current_tick < 30:\n",
        "            return 0.05  # Default volatility when not enough history\n",
        "        \n",
        "        # Use close price changes of first asset as market indicator\n",
        "        window = self.big_pt[self.current_tick-30:self.current_tick, 0, 0, 0]  # Last 30 days\n",
        "        \n",
        "        # Calculate daily returns\n",
        "        returns = np.diff(window) / (window[:-1] + 1e-10)\n",
        "        \n",
        "        # Calculate volatility (standard deviation of returns)\n",
        "        volatility = np.std(returns)\n",
        "        \n",
        "        return volatility\n",
        "\n",
        "    def dynamic_epsilon(self, base=0.5, volatility_scale=1, min_decay=0.995):\n",
        "        market_vol = self.get_30day_volatility()\n",
        "        epsilon = base + (1 - base) * np.tanh(volatility_scale * market_vol)\n",
        "        # Ensure epsilon decays regardless of volatility\n",
        "        self.current_epsilon = min(self.current_epsilon * min_decay, epsilon) \n",
        "        return self.current_epsilon\n",
        "\n",
        "    def phi(self, v):\n",
        "        \"\"\"Operator to increase vector dimension\"\"\"\n",
        "        return np.concatenate(([1], v + 1))\n",
        "\n",
        "    def get_wt_prime_chapeau(self, wt_prime_no_cash, big_s_minus, big_s_plus, pt_prime):\n",
        "        \"\"\"Calculate new weights after action with clearer indexing\"\"\"\n",
        "        wt_prime_chapeau = []\n",
        "        for ind, key in enumerate(wt_prime_no_cash):\n",
        "            # Adjust indices to account for the missing cash position\n",
        "            adj_ind = ind  # The actual asset index in action space\n",
        "            if adj_ind in big_s_minus:\n",
        "                wt_prime_chapeau.append(key - self.delta/pt_prime)\n",
        "            elif adj_ind in big_s_plus:\n",
        "                wt_prime_chapeau.append(key + self.delta/pt_prime)\n",
        "            else:\n",
        "                wt_prime_chapeau.append(key)\n",
        "\n",
        "        return np.array(wt_prime_chapeau)\n",
        "\n",
        "\n",
        "    def is_asset_shortage(self, action, pt, wt):\n",
        "        \"\"\"Check if there's an asset shortage for selling\"\"\"\n",
        "        action = np.atleast_1d(action)\n",
        "        big_s_minus = np.where(action == 0)[0]\n",
        "        for ind in big_s_minus:\n",
        "            if wt[ind+1] * pt < self.delta:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def is_cash_shortage(self, action, pt, wt):\n",
        "        \"\"\"Check if there's a cash shortage for buying\"\"\"\n",
        "        action = np.atleast_1d(action)\n",
        "        big_s_minus = np.where(action == 0)[0]\n",
        "        big_s_plus = np.where(action == 2)[0]\n",
        "        current_cash = wt[0] * pt\n",
        "        cash_after_selling = current_cash + (1 - self.c_minus) * self.delta * len(big_s_minus)\n",
        "        cash_needed = (self.c_plus + 1) * self.delta * len(big_s_plus)\n",
        "\n",
        "        if cash_after_selling < cash_needed:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def action_mapping(self, action, action_Q_values, pt, wt):\n",
        "        \"\"\"Implementation of Algorithm 1 from the paper\"\"\"\n",
        "        action = copy(action)\n",
        "    \n",
        "        # Check for asset shortage first (as per Algorithm 1)\n",
        "        if self.is_asset_shortage(action, pt, wt):\n",
        "            action_mapped = self.rule2(action, action_Q_values, pt, wt)\n",
        "        # Then check for cash shortage\n",
        "        elif self.is_cash_shortage(action, pt, wt):\n",
        "            action_mapped = self.rule1(action, action_Q_values, pt, wt)\n",
        "        else:\n",
        "            action_mapped = action\n",
        "        \n",
        "        return action_mapped\n",
        "\n",
        "    def rule1(self, action, action_Q_values, pt, wt):\n",
        "        \"\"\"Rule 1: Handle cash shortage case\"\"\"\n",
        "        MAXQ = -np.inf\n",
        "        action_selected = action\n",
        "    \n",
        "        # Identify assets to buy in this action\n",
        "        big_s_plus = np.where(action == 2)[0]\n",
        "    \n",
        "        # Generate power set of buying assets (all possible subsets)\n",
        "        power_set = []\n",
        "        for i in range(len(big_s_plus) + 1):\n",
        "            for c in combinations(big_s_plus, i):\n",
        "                power_set.append(c)\n",
        "    \n",
        "        # Test each subset\n",
        "        for subset in power_set:\n",
        "            new_action = copy(action)\n",
        "            # Convert buy actions to hold for assets in this subset\n",
        "            for j in subset:\n",
        "                new_action[j] = 1  # Change to hold\n",
        "        \n",
        "            # Check if this new action is feasible\n",
        "            if not self.is_cash_shortage(new_action, pt, wt):\n",
        "                # Get Q-value for this action\n",
        "                new_action_index = self.find_action_index(new_action)\n",
        "                if new_action_index is not None:\n",
        "                    new_action_Q_value = action_Q_values[new_action_index]\n",
        "                \n",
        "                    # Update if this is the best action so far\n",
        "                    if new_action_Q_value > MAXQ:\n",
        "                        MAXQ = new_action_Q_value\n",
        "                        action_selected = new_action\n",
        "    \n",
        "        return action_selected\n",
        "\n",
        "    def rule2(self, action, action_Q_values, pt, wt):\n",
        "        \"\"\"Rule 2: Handle asset shortage case\"\"\"\n",
        "        # Convert sell actions to hold for assets with shortage\n",
        "        for i in range(len(action)):\n",
        "            if action[i] == 0:  # Sell action\n",
        "                if wt[i+1] * pt < self.delta:  # Check if asset shortage\n",
        "                    action[i] = 1  # Change to hold\n",
        "    \n",
        "        # After fixing asset shortage, check if there's now a cash shortage\n",
        "        if self.is_cash_shortage(action, pt, wt):\n",
        "            action = self.rule1(action, action_Q_values, pt, wt)\n",
        "    \n",
        "        return action\n",
        "\n",
        "    def simulate_all_feasible_actions(self, state):\n",
        "        \"\"\"Simulate all feasible actions in parallel as described in Section 4.2\"\"\"\n",
        "        experiences = []\n",
        "    \n",
        "        # Skip simulation if we're at or near the end of our data\n",
        "        # This prevents trying to access indices beyond our data boundaries\n",
        "        if self.current_tick >= len(self.big_pt) - 2:\n",
        "            return experiences\n",
        "    \n",
        "        # Get all feasible actions for current state\n",
        "        feasible_actions = self.F(state['pt'], state['wt_prime'])\n",
        "    \n",
        "        # Simulate each feasible action\n",
        "        for action_idx in feasible_actions:\n",
        "            action = self.actions[action_idx]\n",
        "            next_state, reward, done, pt_next, wt_next = self.step(action, simulation=True)\n",
        "            next_state['pt'] = pt_next\n",
        "            experiences.append((state, action, reward, next_state, done))\n",
        "    \n",
        "        return experiences\n",
        "\n",
        "    def track_portfolio_composition(self):\n",
        "        \"\"\"Return current portfolio composition for debugging\"\"\"\n",
        "        composition = {\n",
        "            'day': self.current_tick,\n",
        "            'cash_pct': self.wt[0] * 100,\n",
        "            'asset_weights': self.wt[1:] * 100,\n",
        "            'portfolio_value': self.pt\n",
        "        }\n",
        "        return composition\n",
        "\n",
        "    def F(self, pt, wt):\n",
        "        \"\"\"Get all feasible actions in current state\"\"\"\n",
        "        action_possible = []\n",
        "        for ind, action in enumerate(self.actions):\n",
        "            if not self.is_asset_shortage(action, pt, wt) and not self.is_cash_shortage(action, pt, wt):\n",
        "                action_possible.append(ind)\n",
        "        return np.array(action_possible)\n",
        "\n",
        "    def step(self, action, simulation=False):\n",
        "        # Check if episode is ending\n",
        "        if self.current_tick == len(self.big_pt) - 2:\n",
        "            self.episode_ended = True\n",
        "\n",
        "        # Step 1: Calculate portfolio value and weights after state evolution but before action\n",
        "        ktc = self.big_pt[self.current_tick, 0, :, 0]  # Fixed index to 0\n",
        "        pt_prime = self.pt * np.dot(self.wt, self.phi(ktc))\n",
        "        wt_prime = (self.wt * self.phi(ktc)) / (np.dot(self.wt, self.phi(ktc)))\n",
        "\n",
        "        # Step 2: Apply the action\n",
        "        action = np.atleast_1d(action)\n",
        "        big_s_minus = np.where(action == 0)[0]\n",
        "        big_s_plus = np.where(action == 2)[0]\n",
        "\n",
        "        # Step 3: Calculate transaction costs\n",
        "        ct = (self.delta * (self.c_minus * len(big_s_minus) + self.c_plus * len(big_s_plus))) / pt_prime\n",
        "        \n",
        "        if not simulation:\n",
        "            pt_after_costs = pt_prime * (1 - ct)\n",
        "        else:\n",
        "            pt_after_costs = pt_prime * (1 - ct)\n",
        "\n",
        "        # Step 4: Calculate new weights after action\n",
        "        wt_prime_chapeau = self.get_wt_prime_chapeau(wt_prime[1:], big_s_minus, big_s_plus, pt_prime)\n",
        "        wt_prime_chapeau_0 = wt_prime[0] + self.delta * ((1 - self.c_minus) * len(big_s_minus) - \n",
        "                                                         (1 + self.c_plus) * len(big_s_plus)) / pt_prime\n",
        "        wt_prime_chapeau = np.concatenate((np.array([wt_prime_chapeau_0]), wt_prime_chapeau))\n",
        "\n",
        "        # Step 5: Apply weight normalization and prepare for next state\n",
        "        if not simulation:\n",
        "            # Clip any negative weights to 0\n",
        "            if all(action == 1):  # Hold action\n",
        "                self.wt = wt_prime_chapeau  # Let weights drift naturally\n",
        "            else:\n",
        "                self.wt = wt_prime_chapeau / np.sum(wt_prime_chapeau)\n",
        "            # Renormalize so that the sum equals 1\n",
        "            self.wt = (self.wt * self.phi(ktc)) / np.dot(self.wt, self.phi(ktc))\n",
        "            self.current_tick += 1\n",
        "            k_t_plus_one_c = self.big_pt[self.current_tick, 0, :, 0]  # Fixed index to 0\n",
        "        else:\n",
        "            if all(action == 1):  # Hold action\n",
        "                self.wt = wt_prime_chapeau  # Let weights drift naturally\n",
        "            else:\n",
        "                self.wt = wt_prime_chapeau / np.sum(wt_prime_chapeau)\n",
        "            current_tick = self.current_tick + 1\n",
        "            k_t_plus_one_c = self.big_pt[current_tick, 0, :, 0]  # Fixed index to 0\n",
        "\n",
        "        # Step 6: Calculate static portfolio value (if no action was taken)\n",
        "        P_t_plus_one_s = pt_prime * np.dot(wt_prime, self.phi(k_t_plus_one_c))\n",
        "        \n",
        "        # Step 7: Calculate actual portfolio value and reward\n",
        "        if not simulation:\n",
        "            p_t_plus_one_prime = pt_after_costs * np.dot(self.wt, self.phi(k_t_plus_one_c))\n",
        "            # Correct reward calculation per the paper\n",
        "            reward = (p_t_plus_one_prime - P_t_plus_one_s) / P_t_plus_one_s\n",
        "            #self.pt = p_t_plus_one_prime\n",
        "            wt_plus_one_prime = (self.wt * self.phi(k_t_plus_one_c)) / (np.dot(self.wt, self.phi(k_t_plus_one_c)))\n",
        "        else:\n",
        "            p_t_plus_one_prime = pt_after_costs * np.dot(self.wt, self.phi(k_t_plus_one_c))\n",
        "            # Correct reward for simulation as well\n",
        "            reward = (p_t_plus_one_prime - P_t_plus_one_s) / P_t_plus_one_s\n",
        "            #pt = p_t_plus_one_prime\n",
        "            wt_plus_one_prime = (self.wt * self.phi(k_t_plus_one_c)) / (np.dot(self.wt, self.phi(k_t_plus_one_c)))\n",
        "        \n",
        "        # Step 8: Return next state, reward, and episode status\n",
        "        if not simulation:\n",
        "            self.pt = p_t_plus_one_prime\n",
        "            return {'big_xt': np.array(self.small_pt[self.current_tick]), \n",
        "                    'wt_prime': wt_plus_one_prime}, reward, self.episode_ended\n",
        "        else:\n",
        "            return {'big_xt': np.array(self.small_pt[current_tick]), \n",
        "                    'wt_prime': wt_plus_one_prime}, reward, self.episode_ended, p_t_plus_one_prime, wt_plus_one_prime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implement Priorotised Buffer Replay Helper classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class SumTree:\n",
        "    \"\"\"Efficient priority storage structure per Park et al. (2020) Appendix B.3\"\"\"\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.tree_depth = int(np.ceil(np.log2(capacity))) + 1\n",
        "        self.tree = np.zeros(2 * capacity - 1)  # SumTree array storage\n",
        "        self.data = np.zeros(capacity, dtype=object)  # Experience storage\n",
        "        self.ptr = 0  # Circular pointer\n",
        "        self.size = 0\n",
        "        \n",
        "    def _propagate(self, idx, delta):\n",
        "        parent = (idx - 1) // 2\n",
        "        # Fix: Ensure delta is a scalar value\n",
        "        if hasattr(delta, 'item') and np.ndim(delta) > 0:\n",
        "            delta = delta.item()\n",
        "        self.tree[parent] += delta\n",
        "        if parent != 0:\n",
        "            self._propagate(parent, delta)\n",
        "            \n",
        "    def _retrieve(self, idx, value):\n",
        "        left = 2 * idx + 1\n",
        "        if left >= len(self.tree):\n",
        "            return idx\n",
        "        if value <= self.tree[left]:\n",
        "            return self._retrieve(left, value)\n",
        "        else:\n",
        "            return self._retrieve(left + 1, value - self.tree[left])\n",
        "        \n",
        "    def total(self):\n",
        "        return self.tree[0]\n",
        "    \n",
        "    def add(self, priority, data):\n",
        "        idx = self.ptr + self.capacity - 1\n",
        "        self.data[self.ptr] = data\n",
        "        self.update(idx, priority)\n",
        "        self.ptr = (self.ptr + 1) % self.capacity\n",
        "        self.size = min(self.size + 1, self.capacity)\n",
        "        \n",
        "    def update(self, idx, priority):\n",
        "        # Fix: Ensure priority is a scalar value\n",
        "        if hasattr(priority, 'item') and np.ndim(priority) > 0:\n",
        "            priority = priority.item()\n",
        "        delta = priority - self.tree[idx]\n",
        "        self.tree[idx] = priority\n",
        "        self._propagate(idx, delta)\n",
        "        \n",
        "    def get(self, value):\n",
        "        idx = self._retrieve(0, value)\n",
        "        data_idx = idx - self.capacity + 1\n",
        "        return (idx, self.tree[idx], self.data[data_idx])\n",
        "\n",
        "class PrioritizedReplayBuffer:\n",
        "    \"\"\"Improved implementation aligned with Park et al. (2020) Section 4.2\"\"\"\n",
        "    def __init__(self, capacity=2000, alpha=0.6, beta=0.4, epsilon=1e-10):\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.epsilon = epsilon\n",
        "        self.max_priority = 1.0\n",
        "        self.tree = SumTree(capacity)\n",
        "        \n",
        "    def _calculate_priority(self, td_error):\n",
        "        return (np.abs(td_error) + self.epsilon) ** self.alpha\n",
        "\n",
        "    def add(self, experience, td_error):\n",
        "        priority = self._calculate_priority(td_error)\n",
        "        self.tree.add(priority, experience)\n",
        "        self.max_priority = max(priority, self.max_priority)\n",
        "\n",
        "    def sample(self, batch_size, beta_increment=None):\n",
        "        if self.tree.size == 0:\n",
        "            return [], [], []\n",
        "\n",
        "        segment = self.tree.total() / batch_size\n",
        "        samples = []\n",
        "        indices = []\n",
        "        priorities = []\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            a = segment * i\n",
        "            b = segment * (i + 1)\n",
        "            value = np.random.uniform(a, b)\n",
        "            idx, priority, data = self.tree.get(value)\n",
        "            samples.append(data)\n",
        "            indices.append(idx)\n",
        "            priorities.append(priority)\n",
        "\n",
        "        # Importance sampling weights\n",
        "        sampling_probs = np.array(priorities) / self.tree.total()\n",
        "        is_weights = np.power(self.tree.size * sampling_probs, -self.beta)\n",
        "        is_weights /= np.max(is_weights)  # Normalize\n",
        "\n",
        "        # Anneal beta\n",
        "        if beta_increment:\n",
        "            self.beta = min(1.0, self.beta + beta_increment)\n",
        "\n",
        "        return samples, indices, is_weights\n",
        "\n",
        "    def update_priorities(self, indices, td_errors):\n",
        "        \"\"\"Update priorities using TD-errors from latest training batch\"\"\"\n",
        "        priorities = self._calculate_priority(td_errors)\n",
        "        for idx, priority in zip(indices, priorities):\n",
        "            self.tree.update(idx, priority)\n",
        "        self.max_priority = max(priorities.max(), self.max_priority)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You Surely, have to track your metrics!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MetricsTracker:\n",
        "    \"\"\"Class to track and calculate statistics for various training metrics\"\"\"\n",
        "    def __init__(self):\n",
        "        self.metrics = defaultdict(list)\n",
        "        self.episode_metrics = defaultdict(list)\n",
        "        \n",
        "    def add(self, metric_name, value):\n",
        "        \"\"\"Add a value to the specified metric\"\"\"\n",
        "        self.episode_metrics[metric_name].append(value)\n",
        "        \n",
        "    def log_episode(self, episode, year):\n",
        "        \"\"\"Calculate statistics for episode metrics and log them\"\"\"\n",
        "        # Calculate statistics for episode metrics\n",
        "        for metric, values in self.episode_metrics.items():\n",
        "            if len(values) > 0:\n",
        "                # Store episode statistics in overall metrics\n",
        "                self.metrics[f\"{metric}_mean\"].append(np.mean(values))\n",
        "                if len(values) > 1:\n",
        "                    self.metrics[f\"{metric}_std\"].append(np.std(values))\n",
        "                else:\n",
        "                    self.metrics[f\"{metric}_std\"].append(0)\n",
        "        \n",
        "        # Store episode number and year\n",
        "        self.metrics[\"episode\"].append(episode)\n",
        "        self.metrics[\"year\"].append(year)\n",
        "        \n",
        "        # Reset episode metrics for next episode\n",
        "        self.episode_metrics = defaultdict(list)\n",
        "    \n",
        "    def get_dataframe(self):\n",
        "        \"\"\"Convert metrics to pandas DataFrame\"\"\"\n",
        "        return pd.DataFrame(self.metrics)\n",
        "    \n",
        "    def plot_metrics(self, save_path=\"training_metrics.png\"):\n",
        "        \"\"\"Plot key metrics and save to file\"\"\"\n",
        "        metrics_df = self.get_dataframe()\n",
        "        \n",
        "        fig, axes = plt.subplots(4, 2, figsize=(15, 20))\n",
        "        fig.suptitle('DQN Portfolio Training Metrics', fontsize=16)\n",
        "        \n",
        "        # Plot metrics\n",
        "        if 'cumulative_return' in metrics_df:\n",
        "            axes[0, 0].plot(metrics_df['episode'], metrics_df['cumulative_return'])\n",
        "            axes[0, 0].set_title('Cumulative Return (%)')\n",
        "            axes[0, 0].set_xlabel('Episode')\n",
        "            axes[0, 0].set_ylabel('Return (%)')\n",
        "        \n",
        "        if 'q_values_mean' in metrics_df:\n",
        "            axes[0, 1].plot(metrics_df['episode'], metrics_df['q_values_mean'])\n",
        "            axes[0, 1].set_title('Average Q-values')\n",
        "            axes[0, 1].set_xlabel('Episode')\n",
        "            axes[0, 1].set_ylabel('Q-value')\n",
        "            \n",
        "        if 'reward_mean' in metrics_df:\n",
        "            axes[1, 0].plot(metrics_df['episode'], metrics_df['reward_mean'])\n",
        "            axes[1, 0].set_title('Average Reward')\n",
        "            axes[1, 0].set_xlabel('Episode')\n",
        "            axes[1, 0].set_ylabel('Reward')\n",
        "            \n",
        "        if 'epsilon' in metrics_df:\n",
        "            axes[1, 1].plot(metrics_df['episode'], metrics_df['epsilon'])\n",
        "            axes[1, 1].set_title('Epsilon (Exploration Rate)')\n",
        "            axes[1, 1].set_xlabel('Episode')\n",
        "            axes[1, 1].set_ylabel('Epsilon')\n",
        "            \n",
        "        if 'position_change_mean' in metrics_df:\n",
        "            axes[2, 0].plot(metrics_df['episode'], metrics_df['position_change_mean'])\n",
        "            axes[2, 0].set_title('Average Position Change')\n",
        "            axes[2, 0].set_xlabel('Episode')\n",
        "            axes[2, 0].set_ylabel('Change')\n",
        "            \n",
        "        if 'loss_mean' in metrics_df and 'loss_std' in metrics_df:\n",
        "            axes[2, 1].errorbar(metrics_df['episode'], metrics_df['loss_mean'], \n",
        "                              yerr=metrics_df['loss_std'], alpha=0.7)\n",
        "            axes[2, 1].set_title('Loss (Mean ± Std)')\n",
        "            axes[2, 1].set_xlabel('Episode')\n",
        "            axes[2, 1].set_ylabel('Loss')\n",
        "            \n",
        "        if 'action_buy' in metrics_df and 'action_sell' in metrics_df and 'action_hold' in metrics_df:\n",
        "            axes[3, 0].plot(metrics_df['episode'], metrics_df['action_buy'], label='Buy')\n",
        "            axes[3, 0].plot(metrics_df['episode'], metrics_df['action_sell'], label='Sell')\n",
        "            axes[3, 0].plot(metrics_df['episode'], metrics_df['action_hold'], label='Hold')\n",
        "            axes[3, 0].set_title('Action Counts')\n",
        "            axes[3, 0].set_xlabel('Episode')\n",
        "            axes[3, 0].set_ylabel('Count')\n",
        "            axes[3, 0].legend()\n",
        "            \n",
        "        if 'transaction_cost_mean' in metrics_df:\n",
        "            axes[3, 1].plot(metrics_df['episode'], metrics_df['transaction_cost_mean'])\n",
        "            axes[3, 1].set_title('Average Transaction Costs')\n",
        "            axes[3, 1].set_xlabel('Episode')\n",
        "            axes[3, 1].set_ylabel('Cost')\n",
        "            \n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "        \n",
        "        print(f\"Training metrics plotted and saved to {save_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the Model and Trainning Step Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tf.keras.utils.register_keras_serializable()\n",
        "class TransposeLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, perm=[0, 2, 1], **kwargs):\n",
        "        super(TransposeLayer, self).__init__(**kwargs)\n",
        "        self.perm = perm\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        return tf.transpose(inputs, perm=self.perm)\n",
        "    \n",
        "    def get_config(self):\n",
        "        config = super(TransposeLayer, self).get_config()\n",
        "        config.update({\"perm\": self.perm})\n",
        "        return config\n",
        "\n",
        "# Then modify your build_model function to use the custom layer\n",
        "def build_model(datasets, action_shape):\n",
        "    # Modified encoder with custom transpose layer\n",
        "    shared_encoder = tf.keras.models.Sequential([\n",
        "        TransposeLayer(name=\"transpose_layer\"),  # Custom layer instead of Lambda\n",
        "        tf.keras.layers.LSTM(units=128, return_sequences=True, name=\"shared_lstm1\"),\n",
        "        tf.keras.layers.LSTM(units=20, name=\"shared_lstm2\")\n",
        "    ])\n",
        "\n",
        "    # Portfolio weights input\n",
        "    wt_prime_input = tf.keras.layers.Input(shape=[4], name=\"weight_input\")\n",
        "    \n",
        "    # Technical indicator inputs for each asset\n",
        "    asset_inputs = []\n",
        "    encoded_assets = []\n",
        "    \n",
        "    # Apply the SAME encoder to each asset - now accepting (5, 20) format\n",
        "    for i, asset in enumerate(datasets):\n",
        "        asset_input = tf.keras.layers.Input(shape=[5, 20], name=f\"asset_{i}_input\")\n",
        "        asset_inputs.append(asset_input)\n",
        "        \n",
        "        # Use shared encoder with built-in transpose\n",
        "        encoded = shared_encoder(asset_input)\n",
        "        encoded_assets.append(encoded)\n",
        "    \n",
        "    # Rest of the model remains the same\n",
        "    encoded_features = tf.keras.layers.Concatenate(axis=1)(encoded_assets)\n",
        "    combined_features = tf.keras.layers.Concatenate(axis=1)([encoded_features, wt_prime_input])\n",
        "    \n",
        "    hidden1 = tf.keras.layers.Dense(\n",
        "        256, activation=\"relu\",\n",
        "        kernel_regularizer=l2(0.001),\n",
        "        activity_regularizer=l1(0.0005),\n",
        "        name=\"hidden1\"\n",
        "    )(combined_features)\n",
        "    \n",
        "    hidden1 = LayerNormalization()(hidden1)\n",
        "    hidden1 = tf.keras.layers.Dropout(0.2)(hidden1)\n",
        "    \n",
        "    hidden2 = tf.keras.layers.Dense(\n",
        "        64, activation=\"tanh\", \n",
        "        kernel_constraint=unit_norm(),\n",
        "        name=\"hidden2\"\n",
        "    )(hidden1)\n",
        "    \n",
        "    output = tf.keras.layers.Dense(\n",
        "        action_shape,\n",
        "        name=\"q_values\"\n",
        "    )(hidden2)\n",
        "    \n",
        "    # Create model\n",
        "    model = tf.keras.models.Model(inputs=[wt_prime_input] + asset_inputs, outputs=output)\n",
        "    return model\n",
        "\n",
        "@tf.function\n",
        "def train_step(asset1, asset2, asset3, wts_prime, mask, target_Q_values, weights):\n",
        "    \"\"\"Train step function that works with the modified model\"\"\"\n",
        "    # Cast all inputs to float32 to ensure consistent data types\n",
        "    asset1 = tf.cast(asset1, tf.float32)\n",
        "    asset2 = tf.cast(asset2, tf.float32)\n",
        "    asset3 = tf.cast(asset3, tf.float32)\n",
        "    wts_prime = tf.cast(wts_prime, tf.float32)\n",
        "    mask = tf.cast(mask, tf.float32)\n",
        "    target_Q_values = tf.cast(target_Q_values, tf.float32)\n",
        "    weights = tf.cast(weights, tf.float32)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        # Pass inputs directly to model - no transposition needed\n",
        "        all_Q_values = model([wts_prime, asset1, asset2, asset3], training=True)\n",
        "        \n",
        "        # Calculate masked Q-values and loss\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        td_errors = tf.abs(target_Q_values - Q_values)\n",
        "\n",
        "        # Apply importance sampling weights\n",
        "        loss = tf.reduce_mean(LOSS_FN(target_Q_values, Q_values) * weights)\n",
        "\n",
        "    # Gradient calculation with clipping\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "    \n",
        "    # Handle NaN gradients\n",
        "    gradients = [tf.where(tf.math.is_nan(g), 0.0, g) for g in gradients] \n",
        "    \n",
        "    # Apply gradients to model\n",
        "    OPTIMIZER.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss, td_errors\n",
        "\n",
        "def prepare_inputs_from_state(state):\n",
        "    \"\"\"Convert state dictionary to properly shaped model inputs\"\"\"\n",
        "    wt_prime = np.array([state['wt_prime']])\n",
        "    asset_inputs = []\n",
        "    for i in range(len(datasets)):\n",
        "        # No transpose - provide data in original format (5,20)\n",
        "        asset_data = state['big_xt'][i]\n",
        "        asset_inputs.append(np.array([asset_data]))\n",
        "    return [wt_prime] + asset_inputs\n",
        "\n",
        "def evaluate_model(model, test_env, verbose=True):\n",
        "    \"\"\"Evaluate model performance on test environment\"\"\"\n",
        "    test_state = test_env.reset()\n",
        "    test_state['pt'] = test_env.pt\n",
        "    \n",
        "    initial_portfolio_value = test_env.pt\n",
        "    test_rewards = []\n",
        "    test_portfolio_values = []\n",
        "    test_actions = {'buy': 0, 'sell': 0, 'hold': 0}\n",
        "    test_episode_ended = False\n",
        "    \n",
        "    test_pbar = tqdm(total=235, desc=\"Testing\") if verbose else None\n",
        "    \n",
        "    # Track drawdown\n",
        "    max_test_portfolio_value = initial_portfolio_value\n",
        "    max_test_drawdown = 0\n",
        "    \n",
        "    while not test_episode_ended:\n",
        "        # Use model for action selection (no exploration during testing)\n",
        "        test_state_inputs = prepare_inputs_from_state(test_state)\n",
        "        test_Q_values = model.predict(test_state_inputs, verbose=0)[0]\n",
        "        \n",
        "        # Calculate Q-value statistics\n",
        "        q_mean = np.mean(test_Q_values)\n",
        "        q_std = np.std(test_Q_values)\n",
        "        \n",
        "        # Get best action, apply mapping if needed\n",
        "        best_action_idx = np.argmax(test_Q_values)\n",
        "        best_action = test_env.actions[best_action_idx]\n",
        "        \n",
        "        # Check if best action is feasible\n",
        "        if (not test_env.is_asset_shortage(best_action, test_state['pt'], test_state['wt_prime']) and \n",
        "            not test_env.is_cash_shortage(best_action, test_state['pt'], test_state['wt_prime'])):\n",
        "            test_action = best_action\n",
        "        else:\n",
        "            # Map infeasible action to similar feasible one\n",
        "            test_action = test_env.action_mapping(best_action, test_Q_values, test_state['pt'], test_state['wt_prime'])\n",
        "        \n",
        "        # Count action types\n",
        "        for a in test_action:\n",
        "            if a == 0:\n",
        "                test_actions['sell'] += 1\n",
        "            elif a == 2:\n",
        "                test_actions['buy'] += 1\n",
        "            else:\n",
        "                test_actions['hold'] += 1\n",
        "        \n",
        "        # Take step in test environment\n",
        "        test_next_state, test_reward, test_episode_ended = test_env.step(test_action)\n",
        "        test_next_state['pt'] = test_env.pt\n",
        "        \n",
        "        # Track metrics\n",
        "        test_rewards.append(test_reward)\n",
        "        test_portfolio_values.append(test_env.pt)\n",
        "        \n",
        "        # Track drawdown\n",
        "        if test_env.pt > max_test_portfolio_value:\n",
        "            max_test_portfolio_value = test_env.pt\n",
        "        else:\n",
        "            drawdown = (max_test_portfolio_value - test_env.pt) / max_test_portfolio_value\n",
        "            max_test_drawdown = max(max_test_drawdown, drawdown)\n",
        "        \n",
        "        # Update test state\n",
        "        test_state = test_next_state\n",
        "        \n",
        "        if test_pbar:\n",
        "            test_pbar.update(1)\n",
        "    \n",
        "    if test_pbar:\n",
        "        test_pbar.close()\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    daily_returns = [(v - prev_v) / prev_v for prev_v, v in zip(test_portfolio_values[:-1], test_portfolio_values[1:])]\n",
        "\n",
        "    # Sharpe Ratio calculation\n",
        "    risk_free_rate = 0.0001  # daily risk-free rate (≈2.5% annually)\n",
        "    excess_returns = [r - risk_free_rate for r in daily_returns]\n",
        "    sharpe_ratio = (np.mean(excess_returns) / np.std(excess_returns)) * np.sqrt(252)\n",
        "\n",
        "    # Sterling Ratio calculation\n",
        "    downside_returns = [min(r, 0)**2 for r in daily_returns]\n",
        "    avg_downside = np.sqrt(np.mean(downside_returns))\n",
        "    sterling_ratio = (np.mean(excess_returns) / avg_downside) * np.sqrt(252) if avg_downside > 0 else 0\n",
        "\n",
        "    # Final portfolio value\n",
        "    final_portfolio_value = test_env.pt\n",
        "    portfolio_return = (final_portfolio_value / initial_portfolio_value - 1) * 100\n",
        "    \n",
        "    # Prepare results\n",
        "    results = {\n",
        "        'portfolio_return': portfolio_return,\n",
        "        'final_value': final_portfolio_value,\n",
        "        'sharpe_ratio': sharpe_ratio,\n",
        "        'max_drawdown': max_test_drawdown * 100,\n",
        "        'avg_reward': np.mean(test_rewards),\n",
        "        'std_reward': np.std(test_rewards),\n",
        "        'actions': test_actions,\n",
        "    }\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"Test Results:\")\n",
        "        print(f\"  Portfolio Return: {portfolio_return:.2f}%\")\n",
        "        print(f\"  Final Portfolio Value: ${final_portfolio_value:.2f}\")\n",
        "        print(f\"  Sharpe Ratio: {sharpe_ratio:.4f}\")\n",
        "        print(f\"  Sterling Ratio: {sterling_ratio:.4f}\")\n",
        "        print(f\"  Maximum Drawdown: {max_test_drawdown*100:.2f}%\")\n",
        "        print(f\"  Avg/Std Reward: {np.mean(test_rewards):.4f}/{np.std(test_rewards):.4f}\")\n",
        "        print(f\"  Actions - Buy: {test_actions['buy']}, Sell: {test_actions['sell']}, Hold: {test_actions['hold']}\")\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Main Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main execution code\n",
        "if __name__ == \"__main__\":\n",
        "    # Set seeds for reproducibility\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Define dataset and parameters\n",
        "    datasets = [\"SPY\", \"IWD\", \"IWC\"]\n",
        "    n = 20  # Time window size\n",
        "    \n",
        "    # Training dates (multiple years for better generalization)\n",
        "    train_dates = [\n",
        "        [\"2010-01-01\", \"2010-12-30\"],\n",
        "        [\"2011-01-01\", \"2011-12-30\"],\n",
        "        [\"2012-01-01\", \"2012-12-30\"],\n",
        "        [\"2013-01-01\", \"2013-12-30\"],\n",
        "        [\"2014-01-01\", \"2014-12-30\"],\n",
        "        [\"2015-01-01\", \"2015-12-30\"],\n",
        "    ]\n",
        "    \n",
        "    # Test dates\n",
        "    test_dates = [\n",
        "        [\"2016-01-01\", \"2016-12-30\"],\n",
        "    ]\n",
        "    \n",
        "    # Create environments\n",
        "    train_envs = [PortfolioEnv(d, datasets, n) for d in train_dates]\n",
        "    test_envs = [PortfolioEnv(d, datasets, n) for d in test_dates]\n",
        "    \n",
        "    # Use same scaler for test environments\n",
        "    for test_env in test_envs:\n",
        "        test_env.mm_scaler = train_envs[0].mm_scaler\n",
        "    \n",
        "    # Initialize PrioritizedReplayBuffer (rather than deque)\n",
        "    buffer = PrioritizedReplayBuffer(capacity=2000, alpha=0.6, beta=0.4)\n",
        "    \n",
        "    # Training parameters\n",
        "    BETA = 0.3  # Parameter for truncated geometric distribution sampling\n",
        "    EPOCHS = 50\n",
        "    BATCH_SIZE = 32\n",
        "    DISCOUNT_RATE = 0.9\n",
        "    \n",
        "    # Define loss function and optimizer with reduced learning rate\n",
        "    LR_SCHEDULE = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        1e-4, decay_steps=10000, decay_rate=0.99, staircase=True)\n",
        "    OPTIMIZER = tf.keras.optimizers.Adam(\n",
        "        learning_rate=LR_SCHEDULE, beta_1=0.9, beta_2=0.999, epsilon=1e-7, clipnorm=5)\n",
        "    LOSS_FN = tf.keras.losses.Huber(delta=1)\n",
        "    \n",
        "    # Build model and target model\n",
        "    model = build_model(datasets, train_envs[0].action_shape)\n",
        "    model.summary()\n",
        "    TARGET_MODEL = tf.keras.models.clone_model(model)\n",
        "    TARGET_MODEL.set_weights(model.get_weights())\n",
        "    \n",
        "    # Generate truncated geometric distribution for episode sampling\n",
        "    r = np.random.rand(EPOCHS)\n",
        "    N = len(train_envs)\n",
        "    gen_trunc = (N - 1 - np.floor(np.log(1 - r * (1 - (1 - BETA) ** N)) / np.log(1 - BETA))).astype(int)\n",
        "    metrics_tracker = MetricsTracker()\n",
        "    # Training loop\n",
        "    epsilon_values = []\n",
        "    for epoch, ind_env in enumerate(gen_trunc):\n",
        "        env = train_envs[ind_env]\n",
        "        state = env.reset()\n",
        "        state['pt'] = env.pt\n",
        "        \n",
        "        action_counts = {'buy': 0, 'sell': 0, 'hold': 0}\n",
        "        initial_portfolio_value = env.pt\n",
        "    \n",
        "        pbar = tqdm(total=235, desc=f\"Training Year {ind_env + 2010}, Episode {epoch+1}/{EPOCHS}\")\n",
        "    \n",
        "        episode_reward = 0\n",
        "        episode_ended = False\n",
        "        epoch_epsilon_values = []\n",
        "        while not episode_ended:\n",
        "            # Use dynamic epsilon based on market volatility\n",
        "            EPSILON = max(0.01, env.dynamic_epsilon(base=0.5, volatility_scale=1))\n",
        "            epoch_epsilon_values.append(EPSILON)\n",
        "            metrics_tracker.add('epsilon', EPSILON)\n",
        "            old_weights = env.wt.copy()\n",
        "\n",
        "            # Select action (epsilon-greedy)\n",
        "            if np.random.rand() < EPSILON:\n",
        "                # Random action from feasible set\n",
        "                feasible_actions = env.F(state['pt'], state['wt_prime'])\n",
        "                if len(feasible_actions) > 0:\n",
        "                    action_idx = np.random.choice(feasible_actions)\n",
        "                    action = env.actions[action_idx]\n",
        "                else:\n",
        "                    action = np.ones(len(env.datasets), dtype=int)\n",
        "            else:\n",
        "                # Use model for action selection\n",
        "                state_inputs = prepare_inputs_from_state(state)\n",
        "                Q_values = model.predict(state_inputs, verbose=0)[0]\n",
        "                metrics_tracker.add('q_values', np.mean(Q_values))\n",
        "                # Get best action, apply mapping if needed\n",
        "                best_action_idx = np.argmax(Q_values)\n",
        "                best_action = env.actions[best_action_idx]\n",
        "                \n",
        "                # Check if best action is feasible\n",
        "                if (not env.is_asset_shortage(best_action, state['pt'], state['wt_prime']) and \n",
        "                    not env.is_cash_shortage(best_action, state['pt'], state['wt_prime'])):\n",
        "                    action = best_action\n",
        "                else:\n",
        "                    # Map infeasible action to similar feasible one\n",
        "                    action = env.action_mapping(best_action, Q_values, state['pt'], state['wt_prime'])\n",
        "            for a in action:\n",
        "                if a == 0:  # Sell\n",
        "                    action_counts['sell'] += 1\n",
        "                elif a == 2:  # Buy\n",
        "                    action_counts['buy'] += 1\n",
        "                else:  # Hold\n",
        "                    action_counts['hold'] += 1\n",
        "            # Take step in environment\n",
        "            next_state, reward, episode_ended = env.step(action)\n",
        "            next_state['pt'] = env.pt\n",
        "            metrics_tracker.add('reward', reward)\n",
        "\n",
        "            new_weights = env.wt.copy()\n",
        "            position_change = np.sum(np.abs(new_weights - old_weights))\n",
        "            metrics_tracker.add('position_change', position_change)\n",
        "\n",
        "            # Calculate transaction costs\n",
        "            big_s_minus = np.where(action == 0)[0]\n",
        "            big_s_plus = np.where(action == 2)[0]\n",
        "            transaction_cost = (env.delta * (env.c_minus * len(big_s_minus) + env.c_plus * len(big_s_plus))) / state['pt']\n",
        "            metrics_tracker.add('transaction_cost', transaction_cost)\n",
        "\n",
        "            # Store experience in buffer (with initial priority 1.0)\n",
        "            buffer.add((state, action, reward, next_state, episode_ended), 1.0)\n",
        "            \n",
        "            # Simulate all feasible actions for more experience\n",
        "            all_experiences = env.simulate_all_feasible_actions(state)\n",
        "            for exp in all_experiences:\n",
        "                state_exp, action_exp, reward_exp, next_state_exp, done_exp = exp\n",
        "                buffer.add(exp, 1.0)\n",
        "            \n",
        "            # Once buffer has enough samples, train the model\n",
        "            if buffer.tree.size >= BATCH_SIZE:\n",
        "                # Sample batch with importance sampling weights\n",
        "                batch, indices, weights = buffer.sample(BATCH_SIZE)\n",
        "                \n",
        "                # Unpack batch\n",
        "                states, actions, rewards, next_states, dones = zip(*batch)\n",
        "                \n",
        "                # Prepare batch inputs properly shaped for the model\n",
        "                batch_wt_primes = np.array([s['wt_prime'] for s in states])\n",
        "                batch_preprocessed_states = []\n",
        "                \n",
        "                for i in range(len(datasets)):\n",
        "                    asset_data = []\n",
        "                    for s in states:\n",
        "                        # No transpose - provide data in original format (5,20)\n",
        "                        asset_data.append(s['big_xt'][i])\n",
        "                    batch_preprocessed_states.append(np.array(asset_data))\n",
        "                \n",
        "                batch_preprocessed_states = np.array(batch_preprocessed_states).transpose(1, 0, 2, 3)\n",
        "                \n",
        "                # Create action mask (one-hot encoding)\n",
        "                action_mask = np.zeros((len(batch), env.action_shape))\n",
        "                for i, a in enumerate(actions):\n",
        "                    action_idx = env.find_action_index(a)\n",
        "                    if action_idx is not None:\n",
        "                        action_mask[i, action_idx] = 1\n",
        "                \n",
        "                # Get next state inputs (properly shaped)\n",
        "                next_wt_primes = np.array([s['wt_prime'] for s in next_states])\n",
        "                next_preprocessed_states = []\n",
        "                \n",
        "                # Modified next state preparation without transposition\n",
        "                for i in range(len(datasets)):\n",
        "                    next_asset_data = []\n",
        "                    for s in next_states:\n",
        "                        # No transpose - provide data in original format (5,20)\n",
        "                        next_asset_data.append(s['big_xt'][i])\n",
        "                    next_preprocessed_states.append(np.array(next_asset_data))\n",
        "\n",
        "                \n",
        "                next_preprocessed_states = np.array(next_preprocessed_states).transpose(1, 0, 2, 3)\n",
        "                \n",
        "                # Format next state inputs for model\n",
        "                next_state_inputs = [next_wt_primes]\n",
        "                for i in range(len(datasets)):\n",
        "                    next_state_inputs.append(next_preprocessed_states[:, i])\n",
        "                \n",
        "                # Double DQN: online model selects action, target model estimates value\n",
        "                next_Q_values_online = model.predict(next_state_inputs, verbose=0)\n",
        "                best_next_actions = np.argmax(next_Q_values_online, axis=1)\n",
        "                next_Q_values_target = TARGET_MODEL.predict(next_state_inputs, verbose=0)\n",
        "                \n",
        "                # Calculate target Q-values\n",
        "                target_Q = np.zeros((len(batch), 1))\n",
        "                for i, (r, done, next_action_idx) in enumerate(zip(rewards, dones, best_next_actions)):\n",
        "                    if done:\n",
        "                        target_Q[i] = r\n",
        "                    else:\n",
        "                        target_Q[i] = r + DISCOUNT_RATE * next_Q_values_target[i, next_action_idx]\n",
        "                \n",
        "                # Get each asset's data - NO transposition needed\n",
        "                asset1_data = batch_preprocessed_states[:, 0]  # Shape (batch_size, 5, 20)\n",
        "                asset2_data = batch_preprocessed_states[:, 1]  # Shape (batch_size, 5, 20)\n",
        "                asset3_data = batch_preprocessed_states[:, 2]  # Shape (batch_size, 5, 20)\n",
        "\n",
        "                # Call train_step with raw asset data\n",
        "                loss, td_errors = train_step(\n",
        "                    tf.convert_to_tensor(asset1_data),\n",
        "                    tf.convert_to_tensor(asset2_data),\n",
        "                    tf.convert_to_tensor(asset3_data),\n",
        "                    tf.convert_to_tensor(batch_wt_primes),\n",
        "                    tf.convert_to_tensor(action_mask),\n",
        "                    tf.convert_to_tensor(target_Q),\n",
        "                    tf.convert_to_tensor(weights)\n",
        "                )\n",
        "                metrics_tracker.add('loss', loss.numpy())\n",
        "                \n",
        "                # Update priorities with less restrictive error handling\n",
        "                td_errors = td_errors.numpy()\n",
        "                td_errors = np.clip(td_errors, -10.0, 10.0)  # More generous range\n",
        "                td_errors = np.nan_to_num(td_errors, nan=0.01, posinf=10.0, neginf=-10.0)\n",
        "                buffer.update_priorities(indices, td_errors)\n",
        "            \n",
        "            # Update target network periodically\n",
        "            if (env.current_tick % 2000 == 0):\n",
        "                TARGET_MODEL.set_weights(model.get_weights())\n",
        "            \n",
        "            # Update state for next iteration\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            pbar.update(1)\n",
        "        \n",
        "        pbar.close()\n",
        "        cumulative_return = (env.pt / initial_portfolio_value - 1) * 100\n",
        "        # Log end-of-episode metrics\n",
        "        metrics_tracker.metrics['cumulative_return'].append(cumulative_return)\n",
        "        metrics_tracker.metrics['action_buy'].append(action_counts['buy'])\n",
        "        metrics_tracker.metrics['action_sell'].append(action_counts['sell'])\n",
        "        metrics_tracker.metrics['action_hold'].append(action_counts['hold'])\n",
        "    \n",
        "        # Log episode statistics\n",
        "        metrics_tracker.log_episode(epoch, ind_env + 2010)\n",
        "        avg_epoch_epsilon = np.mean(epoch_epsilon_values)\n",
        "        epsilon_values.append(avg_epoch_epsilon)\n",
        "        # Print summary\n",
        "        print(f\"Episode {epoch+1} average epsilon: {avg_epoch_epsilon:.4f}\")\n",
        "        print(f\"Episode {epoch+1} min/max epsilon: {min(epoch_epsilon_values):.4f}/{max(epoch_epsilon_values):.4f}\")\n",
        "        print(f\"Episode {epoch+1} reward: {episode_reward:.4f}\")\n",
        "        print(f\"Cumulative return: {cumulative_return:.2f}%\")\n",
        "        print(f\"Average Q-value: {metrics_tracker.metrics['q_values_mean'][-1]:.4f} (±{metrics_tracker.metrics['q_values_std'][-1]:.4f})\")\n",
        "        print(f\"Average reward: {metrics_tracker.metrics['reward_mean'][-1]:.4f} (±{metrics_tracker.metrics['reward_std'][-1]:.4f})\")\n",
        "        print(f\"Average position change: {metrics_tracker.metrics['position_change_mean'][-1]:.4f}\")\n",
        "        print(f\"Action counts - Buy: {action_counts['buy']}, Sell: {action_counts['sell']}, Hold: {action_counts['hold']}\")\n",
        "        if 'loss_mean' in metrics_tracker.metrics:\n",
        "            print(f\"Average loss: {metrics_tracker.metrics['loss_mean'][-1]:.4f} (±{metrics_tracker.metrics['loss_std'][-1]:.4f})\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"Episode {epoch+1} reward: {episode_reward}\")\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            # Get the test environment for 2019\n",
        "            test_env = test_envs[0]  # The first test environment is for 2019\n",
        "            test_state = test_env.reset()\n",
        "            test_state['pt'] = test_env.pt\n",
        "        \n",
        "            evaluate_model(model, test_env, verbose=True)\n",
        "\n",
        "\n",
        "    # Save the final model\n",
        "    model.save(\"dqn_portfolio_model.h5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting Portfolio Weights and Portfolio value evolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "def run_and_plot_portfolio_evolution(model, test_env, save_path=\"portfolio_evolution_2017.png\"):\n",
        "    \"\"\"Run the trained model on 2016 data and track portfolio evolution\"\"\"\n",
        "    # Initialize test environment\n",
        "    test_state = test_env.reset()\n",
        "    test_state['pt'] = test_env.pt\n",
        "    \n",
        "    # Track metrics\n",
        "    initial_portfolio_value = test_env.pt\n",
        "    portfolio_track = []\n",
        "    test_episode_ended = False\n",
        "    \n",
        "    # Progress bar for visualization\n",
        "    pbar = tqdm(total=235, desc=\"Testing on 2016 Data\")\n",
        "    \n",
        "    # Run model through entire 2019 trading period\n",
        "    while not test_episode_ended:\n",
        "        # Track current portfolio composition\n",
        "        portfolio_track.append({\n",
        "            'day': test_env.current_tick,\n",
        "            'cash': test_env.wt[0],\n",
        "            'spy': test_env.wt[1],\n",
        "            'iwd': test_env.wt[2],\n",
        "            'iwc': test_env.wt[3],\n",
        "            'portfolio_value': test_env.pt\n",
        "        })\n",
        "        \n",
        "        # Get model prediction for best action\n",
        "        test_state_inputs = prepare_inputs_from_state(test_state)\n",
        "        test_Q_values = model.predict(test_state_inputs, verbose=0)[0]\n",
        "        \n",
        "        # Select optimal action\n",
        "        best_action_idx = np.argmax(test_Q_values)\n",
        "        best_action = test_env.actions[best_action_idx]\n",
        "        \n",
        "        # Apply action mapping if needed for feasibility\n",
        "        if (not test_env.is_asset_shortage(best_action, test_state['pt'], test_state['wt_prime']) and \n",
        "            not test_env.is_cash_shortage(best_action, test_state['pt'], test_state['wt_prime'])):\n",
        "            test_action = best_action\n",
        "        else:\n",
        "            test_action = test_env.action_mapping(best_action, test_Q_values, test_state['pt'], test_state['wt_prime'])\n",
        "        \n",
        "        # Execute step in environment\n",
        "        test_next_state, test_reward, test_episode_ended = test_env.step(test_action)\n",
        "        test_next_state['pt'] = test_env.pt\n",
        "        \n",
        "        # Update state for next iteration\n",
        "        test_state = test_next_state\n",
        "        pbar.update(1)\n",
        "    \n",
        "    # Add final portfolio state\n",
        "    portfolio_track.append({\n",
        "        'day': test_env.current_tick,\n",
        "        'cash': test_env.wt[0],\n",
        "        'spy': test_env.wt[1],\n",
        "        'iwd': test_env.wt[2],\n",
        "        'iwc': test_env.wt[3],\n",
        "        'portfolio_value': test_env.pt\n",
        "    })\n",
        "    \n",
        "    pbar.close()\n",
        "    \n",
        "    # Convert tracking data to DataFrame\n",
        "    portfolio_df = pd.DataFrame(portfolio_track)\n",
        "    \n",
        "    # Create visualization of portfolio evolution\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 12))\n",
        "    \n",
        "    # Plot weights\n",
        "    ax1.plot(portfolio_df['day'], portfolio_df['cash']*100, label='Cash', linewidth=2, color='blue')\n",
        "    ax1.plot(portfolio_df['day'], portfolio_df['spy']*100, label='Asset 1 (SPY)', linewidth=2, color='green')\n",
        "    ax1.plot(portfolio_df['day'], portfolio_df['iwd']*100, label='Asset 2 (IWD)', linewidth=2, color='orange')\n",
        "    ax1.plot(portfolio_df['day'], portfolio_df['iwc']*100, label='Asset 3 (IWC)', linewidth=2, color='red')\n",
        "    ax1.set_title('Portfolio Weight Evolution (2019)', fontsize=16)\n",
        "    ax1.set_xlabel('Trading Days', fontsize=14)\n",
        "    ax1.set_ylabel('Portfolio Weights (%)', fontsize=14)\n",
        "    ax1.legend(fontsize=12)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot portfolio value\n",
        "    ax2.plot(portfolio_df['day'], portfolio_df['portfolio_value'], linewidth=2, color='green')\n",
        "    ax2.set_title('Portfolio Value Evolution (2019)', fontsize=16)\n",
        "    ax2.set_xlabel('Trading Days', fontsize=14)\n",
        "    ax2.set_ylabel('Value ($)', fontsize=14)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Create secondary y-axis for returns percentage\n",
        "    ax2_2 = ax2.twinx()\n",
        "    initial_value = portfolio_df['portfolio_value'].iloc[0]\n",
        "    returns_pct = [(v/initial_value - 1)*100 for v in portfolio_df['portfolio_value']]\n",
        "    ax2_2.plot(portfolio_df['day'], returns_pct, color='darkgreen', linestyle='--', alpha=0.7)\n",
        "    ax2_2.set_ylabel('Return (%)', fontsize=14, color='darkgreen')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Print summary statistics\n",
        "    final_portfolio_value = test_env.pt\n",
        "    portfolio_return = (final_portfolio_value / initial_portfolio_value - 1) * 100\n",
        "    \n",
        "    print(f\"Final portfolio weights:\")\n",
        "    print(f\"  Cash: {test_env.wt[0]*100:.2f}%\")\n",
        "    print(f\"  SPY: {test_env.wt[1]*100:.2f}%\")\n",
        "    print(f\"  IWD: {test_env.wt[2]*100:.2f}%\")\n",
        "    print(f\"  IWC: {test_env.wt[3]*100:.2f}%\")\n",
        "    print(f\"Portfolio Return: {portfolio_return:.2f}%\")\n",
        "    print(f\"Final Portfolio Value: ${final_portfolio_value:.2f}\")\n",
        "    \n",
        "    # Save portfolio data for further analysis\n",
        "    portfolio_df.to_csv(f\"portfolio_evolution_2019_{datetime.datetime.now().strftime('%Y%m%d')}.csv\", index=False)\n",
        "    \n",
        "    return portfolio_df\n",
        "\n",
        "def calculate_buy_and_hold(test_env):\n",
        "    # Extract price data for the assets\n",
        "    test_env.reset()\n",
        "    spy_prices = test_env.final_df['SPY_close'].values\n",
        "    iwd_prices = test_env.final_df['IWD_close'].values\n",
        "    iwc_prices = test_env.final_df['IWC_close'].values\n",
        "\n",
        "    # Initial portfolio value and equal allocation\n",
        "    initial_total = 1e6\n",
        "    spy_shares = (initial_total / 3) / spy_prices[0]\n",
        "    iwd_shares = (initial_total / 3) / iwd_prices[0]\n",
        "    iwc_shares = (initial_total / 3) / iwc_prices[0]\n",
        "\n",
        "    # Calculate portfolio value over time\n",
        "    bh_values = []\n",
        "    for spy_price, iwd_price, iwc_price in zip(spy_prices, iwd_prices, iwc_prices):\n",
        "        current_value = (spy_shares * spy_price) + (iwd_shares * iwd_price) + (iwc_shares * iwc_price)\n",
        "        bh_values.append(current_value)\n",
        "\n",
        "    return bh_values[:len(test_env.final_df)]  # Ensure alignment with DQN data\n",
        "def plot_portfolio_evolution_with_bh(portfolio_df, bh_values):\n",
        "    # Ensure dimensions match\n",
        "    if len(portfolio_df) != len(bh_values):\n",
        "        bh_values = bh_values[:len(portfolio_df)]\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(figsize=(14, 7))\n",
        "\n",
        "    # Plot DQN Portfolio\n",
        "    ax.plot(portfolio_df['day'], portfolio_df['portfolio_value'], label='DQN Portfolio', color='green', linewidth=2)\n",
        "\n",
        "    # Plot Buy and Hold\n",
        "    ax.plot(portfolio_df['day'], bh_values, label='Buy & Hold', color='blue', linestyle='--', linewidth=2)\n",
        "\n",
        "    # Add titles and labels\n",
        "    ax.set_title('Portfolio Value Comparison (DQN vs. Buy & Hold)', fontsize=16)\n",
        "    ax.set_xlabel('Trading Days', fontsize=14)\n",
        "    ax.set_ylabel('Portfolio Value ($)', fontsize=14)\n",
        "    ax.legend(fontsize=12)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Show the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Main execution code\n",
        "if __name__ == \"__main__\":\n",
        "    # Assumes the following variables are already defined:\n",
        "    # - model: the trained DQN model\n",
        "    # - test_envs: list of test environments where test_envs[0] is 2019 data\n",
        "    # - prepare_inputs_from_state: function to format state for model input\n",
        "    \n",
        "    print(\"Running trained DQN model on 2017 test data...\")\n",
        "    test_dates = [\n",
        "    [\"2016-01-01\", \"2016-12-30\"]]\n",
        "    test_envs = [PortfolioEnv(d, datasets, n) for d in test_dates]\n",
        "    portfolio_data = run_and_plot_portfolio_evolution(model, test_envs[0])\n",
        "    \n",
        "    # For additional analysis, you could calculate:\n",
        "    # 1. Trading frequency analysis\n",
        "    buy_count = ((portfolio_data['spy'].diff() > 0) | \n",
        "                 (portfolio_data['iwd'].diff() > 0) | \n",
        "                 (portfolio_data['iwc'].diff() > 0)).sum()\n",
        "    \n",
        "    sell_count = ((portfolio_data['spy'].diff() < 0) | \n",
        "                  (portfolio_data['iwd'].diff() < 0) | \n",
        "                  (portfolio_data['iwc'].diff() < 0)).sum()\n",
        "    bh_values = calculate_buy_and_hold(test_envs[0])\n",
        "\n",
        "    # Plot DQN Portfolio vs. Buy & Hold Strategy\n",
        "    plot_portfolio_evolution_with_bh(portfolio_data, bh_values)\n",
        "    \n",
        "    print(f\"\\nTrading Analysis:\")\n",
        "    print(f\"  Buy actions: {buy_count}\")\n",
        "    print(f\"  Sell actions: {sell_count}\")\n",
        "    print(f\"  Trading activity rate: {(buy_count + sell_count)/len(portfolio_data)*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computing Sterling Ratio and Sharpe Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the final model on the test environment and print metrics\n",
        "results = evaluate_model(model, test_envs[0], verbose=False)\n",
        "\n",
        "print(\"Final Model Performance on Test Environment:\")\n",
        "print(f\"Portfolio Return: {results['portfolio_return']:.2f}%\")\n",
        "print(f\"Final Portfolio Value: ${results['final_value']:.2f}\")\n",
        "print(f\"Sharpe Ratio (from evaluate_model): {results['sharpe_ratio']:.4f}\")\n",
        "\n",
        "# Re-run simulation on test_env to record portfolio evolution and compute ratios from scratch\n",
        "state = test_envs[0].reset()\n",
        "state['pt'] = test_envs[0].pt\n",
        "portfolio_values = [test_envs[0].pt]\n",
        "\n",
        "while True:\n",
        "    # Use the model to determine best action without exploration\n",
        "    state_inputs = prepare_inputs_from_state(state)\n",
        "    Q_values = model.predict(state_inputs, verbose=0)[0]\n",
        "    best_action_idx = np.argmax(Q_values)\n",
        "    best_action = test_envs[0].actions[best_action_idx]\n",
        "    \n",
        "    # If best action is not feasible, map it to a feasible one\n",
        "    if (not test_envs[0].is_asset_shortage(best_action, state['pt'], state['wt_prime']) and \n",
        "        not test_envs[0].is_cash_shortage(best_action, state['pt'], state['wt_prime'])):\n",
        "        action = best_action\n",
        "    else:\n",
        "        action = test_envs[0].action_mapping(best_action, Q_values, state['pt'], state['wt_prime'])\n",
        "    \n",
        "    next_state, reward, done = test_envs[0].step(action)\n",
        "    next_state['pt'] = test_envs[0].pt\n",
        "    portfolio_values.append(test_envs[0].pt)\n",
        "    \n",
        "    state = next_state\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "portfolio_values = np.array(portfolio_values)\n",
        "daily_returns = np.diff(portfolio_values) / portfolio_values[:-1]\n",
        "risk_free_rate = 0.0001  # daily risk-free rate\n",
        "excess_returns = daily_returns - risk_free_rate\n",
        "\n",
        "# Calculate Sharpe ratio\n",
        "computed_sharpe = (np.mean(excess_returns) / np.std(excess_returns)) * np.sqrt(252)\n",
        "\n",
        "# Calculate Sterling ratio\n",
        "downside_returns = np.array([min(r, 0) ** 2 for r in daily_returns])\n",
        "avg_downside = np.sqrt(np.mean(downside_returns))\n",
        "computed_sterling = (np.mean(excess_returns) / avg_downside) * np.sqrt(252) if avg_downside > 0 else 0\n",
        "\n",
        "print(f\"Recomputed Sharpe Ratio: {computed_sharpe:.4f}\")\n",
        "print(f\"Sterling Ratio: {computed_sterling:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computing B&H Returns for all 4 years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the date ranges for each year\n",
        "years = {\n",
        "    \"2016\": [\"2016-01-01\", \"2016-12-30\"],\n",
        "    \"2017\": [\"2017-01-01\", \"2017-12-30\"],\n",
        "    \"2018\": [\"2018-01-01\", \"2018-12-30\"],\n",
        "    \"2019\": [\"2019-01-01\", \"2019-12-30\"],\n",
        "}\n",
        "\n",
        "# Dictionary to hold results\n",
        "results = {}\n",
        "\n",
        "# Loop through each year\n",
        "for yr, dates in years.items():\n",
        "    print(f\"\\nYear {yr}:\")\n",
        "    # Create a PortfolioEnv for the given year (datasets and n are defined in earlier cells)\n",
        "    env = PortfolioEnv(dates, datasets, n)\n",
        "    \n",
        "    # Calculate Buy & Hold portfolio values over the period\n",
        "    env.reset()\n",
        "    env.wt = np.array([0.25, 0.25, 0.25, 0.25])  # Start with all cash\n",
        "    bh_values = calculate_buy_and_hold(env)\n",
        "    \n",
        "    # Compute overall return (%)\n",
        "    initial_value = bh_values[0]\n",
        "    final_value = bh_values[-1]\n",
        "    return_pct = (final_value / initial_value - 1) * 100\n",
        "    \n",
        "    # Compute daily returns\n",
        "    bh_values_arr = np.array(bh_values)\n",
        "    daily_returns = np.diff(bh_values_arr) / bh_values_arr[:-1]\n",
        "    \n",
        "    # Set risk free rate (daily)\n",
        "    risk_free_rate = 0.0001  # daily risk-free rate\n",
        "    \n",
        "    # Excess daily returns\n",
        "    excess_returns = daily_returns - risk_free_rate\n",
        "    \n",
        "    # Sharpe Ratio (annualized)\n",
        "    if np.std(excess_returns) > 0:\n",
        "        sharpe = (np.mean(excess_returns) / np.std(excess_returns)) * np.sqrt(252)\n",
        "    else:\n",
        "        sharpe = 0.0\n",
        "    \n",
        "    # Sterling Ratio: calculate downside risk\n",
        "    downside_returns = np.array([min(r, 0)**2 for r in daily_returns])\n",
        "    avg_downside = np.sqrt(np.mean(downside_returns))\n",
        "    if avg_downside > 0:\n",
        "        sterling = (np.mean(excess_returns) / avg_downside) * np.sqrt(252)\n",
        "    else:\n",
        "        sterling = 0.0\n",
        "    \n",
        "    # Save results\n",
        "    results[yr] = {\n",
        "        \"Return (%)\": return_pct,\n",
        "        \"Final Value\": final_value,\n",
        "        \"Sharpe Ratio\": sharpe,\n",
        "        \"Sterling Ratio\": sterling\n",
        "    }\n",
        "    \n",
        "    # Print the results\n",
        "    print(f\"  Buy & Hold Return: {return_pct:.2f}%\")\n",
        "    print(f\"  Final Portfolio Value: ${final_value:,.2f}\")\n",
        "    print(f\"  Sharpe Ratio: {sharpe:.4f}\")\n",
        "    print(f\"  Sterling Ratio: {sterling:.4f}\")\n",
        "\n",
        "# Optionally, print the dictionary containing all results.\n",
        "print(\"\\nSummary:\")\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
